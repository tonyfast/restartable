{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Procedural Notebooks\n",
    "\n",
    "                 \n",
    "_Typically_ a notebook's author will begin an idea from a blank documents in an editable state.  Through cycles of __interactive__ computing an author will transform the notebook's data by adding narrative, code, and metadata.  The notebook's cells are __parts__ of a __whole__ computable document described by the notebook format.\n",
    "\n",
    "The __interactive__ _in-memory_ editing mode is a critical, but fleeting stage in the life of a computable document.  Notebooks spend most of their existence as __whole__ & __static__ files _on disk_. The __static__ state of notebooks are reusable; and for notebooks to be reusable they must be reused.  \n",
    "\n",
    "__Procedural__ notebooks are readable and reusable literate documents that can be executed successfully in other contexts like documention, module development, or external jobs.  This notebook explores the reusability of __procedural__ notebooks that \n",
    "successfully _Restart and Run All_.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This literate document can be viewed as a [notebook](http://nbviewer.jupyter.org/github/tonyfast/restartable/blob/master/readme.ipynb), <a href=\"http://nbviewer.jupyter.org/format/slides/github/tonyfast/restartable/blob/master/readme.ipynb\" title=\"View as Slides\"><span class=\"menu-text\">presentation</span><span class=\"fa fa-gift fa-2x menu-icon\"></span></a>, or <a href=\"https://github.com/tonyfast/restartable/blob/master/readme.ipynb\" title=\"View on GitHub\"><span class=\"menu-text\">View on GitHub</span><span class=\"fa fa-github fa-2x menu-icon\"></span></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Motivation\n",
    "\n",
    "__Procedural__ notebooks are inspired by [Paco Nathan](http://liber118.com/pxn/)'s [_Oriole: a new learning medium based on Jupyter + Docker_](http://nbviewer.jupyter.org/github/jupyterday-atlanta-2016/oriole_jupyterday_atl/blob/master/oriole_talk.ipynb) given at [Jupyter Day Atlanta 2016](https://jupyterday-atlanta-2016.github.io). In Paco's _unofficial_ [styleguide for authoring Jupyter notebooks](http://nbviewer.jupyter.org/github/jupyterday-atlanta-2016/oriole_jupyterday_atl/blob/master/oriole_talk.ipynb#What-we-learned-about-teaching-with-notebooks) he suggests:\n",
    "\n",
    "> clear all output then \"Run All\" -- or it didn't happen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## __Procedural notebooks__\n",
    "\n",
    "* ... restart and run all, or they don't.  Their reusability can be tested in different contexts.\n",
    "* ... change over time\n",
    "* ... encapsulate cycles of [non-structured](https://en.wikipedia.org/wiki/Non-structured_programming),\n",
    "[structured](https://en.wikipedia.org/wiki/Structured_programming), and [literate programming](https://en.wikipedia.org/wiki/Literate_programming) actions.\n",
    "* ... can be executed in other contexts like testing, document conversion, or compute...\n",
    "* ... can be tested as __parts__ in __interactive__ mode\n",
    "* ... can be tested as a __whole__ in a __procedural__ mode\n",
    "* ... may be used to create sophisticated software projects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# This notebook is procedural notebook\n",
    "\n",
    "Its cells _Restart and Run All_ to create a module and python package called __particles__:\n",
    "\n",
    "* Create, describe, and test source code for a project we call __particles__.\n",
    "* Copy the source code to a notebook called __particles.ipynb__\n",
    "* Convert __particles.ipynb__ to __particles.py__ and a Python package called __particles__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > __particles__ is inspired by the New York Times R&D [_The Future of News is Not an Article_](http://nytlabs.com/blog/2015/10/20/particles/). __particles__ treat elements of computable documents as data and modular components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Procedurally create the particles module\n",
    "\n",
    "__readme.ipynb__ generates the __particles__ module either in  interactive mode, or procedurally from a converted Python script.  \n",
    "\n",
    "`attach` is a callable used by __readme__ to append the recent `In`put as cell source to __particles.ipynb__; _it is erroneous to the __particles__ module.  _If the __readme.ipynb__ cells are run out of order then __particles.ipynb__ could be created incorrectly._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nbformat import v4, NotebookNode\n",
    "nb, particles = 'particles.ipynb', v4.new_notebook();\n",
    "def attach(nb:NotebookNode=particles)->None:\n",
    "    \"\"\"attach an input to another notebook removing attach statements.\n",
    "    >>> nb = v4.new_notebook();\n",
    "    >>> assert attach(nb) or ('doctest' in nb.cells[-1].source)\"\"\"\n",
    "    'In' in globals() and nb.cells.append(v4.new_code_cell('\\n'.join(\n",
    "        str for str in In[-1].splitlines() if not str.startswith('attach'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# build `particles.ipynb`\n",
    "\n",
    "> Many cells in __readme.ipynb__ have lived and died before you read this line.\n",
    "\n",
    "The __code__ cell below will be appended to __particles.ipynb__.  It __import__s tools into __readme.ipynb__'s interactive mode.  It now becomes quite easy to iteratively develop and test __parts__ of the procedural document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'particles treat notebooks as data'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attach(particles)\n",
    "\"\"\"particles treat notebooks as data\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "attach(particles)\n",
    "from nbformat import reads, v4 \n",
    "from pandas import concat, DataFrame, to_datetime \n",
    "from pathlib import Path "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# callables in `particles` \n",
    "\n",
    "Create two main functions for __particles__ to export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "attach()\n",
    "def read_notebooks(dir:str='.')->DataFrame:\n",
    "    \"\"\"Read a directory of notebooks into a pandas.DataFrame\n",
    "    >>> df = read_notebooks('.')\n",
    "    >>> assert len(df) and isinstance(df, DataFrame)\"\"\"\n",
    "    return concat({\n",
    "        file: DataFrame(reads(file.read_text(), 4)['cells'])\n",
    "        for file in Path(dir).glob('*.ipynb')\n",
    "    }).reset_index(-1, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The `read_notebooks` index is a pathlib object containing extra metadata. `files_to_data` extracts the `stat` properties for each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "attach()\n",
    "def files_to_data(df:DataFrame)->DataFrame:\n",
    "    \"\"\"Transform an index of Path's to a dataframe of os_stat.\n",
    "    >>> df = files_to_data(read_notebooks())\n",
    "    \"\"\"\n",
    "    stats, index = [], df.index.unique()\n",
    "    for file in index:\n",
    "        stat = file.stat() \n",
    "        stats.append({\n",
    "            key: to_datetime(\n",
    "                getattr(stat, key), unit=key.endswith('s') and key.rsplit('_')[-1] or 's'\n",
    "            ) if 'time' in key else getattr(stat, key)\n",
    "            for key in dir(stat) if not key.startswith('_') and not callable(getattr(stat, key))})\n",
    "    # Append the change in time to the dataframe.\n",
    "    return DataFrame(stats, index).pipe(lambda df: df.join((df.st_mtime - df.st_birthtime).rename('dt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Control Flow in Procedural Notebooks\n",
    "\n",
    "A procedural notebooks will use clues from a namespace to decide what statements to execute in different contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ != '__main__': assert __name__+'.py' == __file__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### In Jupyter mode\n",
    "\n",
    "> **`__name__`** == **`'__main__'`**, but nothing is known about the python object **`__file__`**.\n",
    "\n",
    "### In Setup mode\n",
    "\n",
    "> **`__name__`** == **`'__main__'`** and **`assert __file__`** .\n",
    "\n",
    "### As a python package mode\n",
    "\n",
    "> **`__name__ + '.py'`** == **`__file__`**.\n",
    "\n",
    "### `get_ipython`\n",
    "\n",
    "The `get_ipython` context must be manually imported to use magics in converted notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    from IPython import get_ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Controlling value assignment\n",
    "\n",
    "Introspect the interactive Jupyter namespace to control expressions in procedural notebooks.\n",
    "\n",
    "    \n",
    "    thing = get_ipython().user_ns.get('thing', 42):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `readme` procedures <small>to make <code>particles</code></small>\n",
    "\n",
    "Below are the procedures to test and create the `particles` package.\n",
    "\n",
    "* [`doctest`](https://docs.python.org/3/library/doctest.html)s were declared in each of our functions.  `doctest` can be run in an interactive notebook session, [`unittest`](https://docs.python.org/3/library/unittest.html) cannot.\n",
    "\n",
    "    `doctest` catches a lot of errors when it is in the _Restart and Run All_ pipeline.  It is a great place to stash repeatedly typed statements.\n",
    "    \n",
    "* When the tests pass write the __particles.ipynb__ notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(__import__('doctest').testmod())\n",
    "    Path(nb).write_text(__import__('nbformat').writes(particles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Transform both __readme.ipynb__ and the newly minted __particles.ipynb__ to python scripts.\n",
    "* Autopep it because we can.\n",
    "* Rerun the same tests on __particles.py__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    !jupyter nbconvert --to python --TemplateExporter.exclude_input_prompt=True particles.ipynb readme.ipynb\n",
    "    !autopep8 --in-place --aggressive readme.py particles.ipynb\n",
    "    !python -m doctest particles.py & echo \"success\"\n",
    "    !jupyter nbconvert --to markdown --TemplateExporter.exclude_input_prompt=True readme.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* `setuptools` will install the __particles__ package  using the conditions for setup mode.  \n",
    "\n",
    "    > Install the __particles__ package\n",
    "    \n",
    "    >    `python readme.py develop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    if __name__ == '__main__' and '__file__' in globals():\n",
    "        __import__('setuptools').setup(\n",
    "            name=\"particles\", \n",
    "            py_modules=['particles'], \n",
    "            install_requires=['notebook', 'pandas'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reusing `particles`\n",
    "\n",
    "__Particles__ can now be imported into the current scope. __particles__ allow the user to explore notebooks and their cells as data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import particles\n",
    "assert particles.__file__.endswith('.py')\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "df = particles.read_notebooks()\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Quantifying lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.source.str.split('\\n').apply(len).groupby([df.index, df.cell_type]).sum().to_frame('lines of ...').unstack(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The distribution of markdown and code cells in the __particles__ project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "    \n",
    "    df.cell_type.groupby(df.index).value_counts().unstack('cell_type').apply(lambda df: df.plot.pie() and plt.show());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "This document must _Restart and Run All_ to acheive the goals of creating the __particles__ module.\n",
    "\n",
    "* __Procedural__ notebooks _Restart and Run All_ or they don't; they can be tested.\n",
    "* Not all notebooks survive, the lucky ones become __procedural__ notebooks.\n",
    "* Literate __procedural__ notebooks reinforce readability and reusability to reproducible work.\n",
    "* __Procedural__ tend to maintain a longer shelf life than an exploratory notebook.\n",
    "* __Interactive__ programming is complex and an author will rely on multiple styles of programming to acheive a __procedural__ document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
